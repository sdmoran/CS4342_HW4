{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "import pandas\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** No CODEPAGE record, no encoding_override: will use 'ascii'\n",
      "Confusion matrix: \n",
      "[[11  1]\n",
      " [ 1 12]]\n",
      "Log Likelihood:  -2.769951122944561\n",
      "Confusion matrix: \n",
      "[[10  2]\n",
      " [ 2 11]]\n",
      "Log Likelihood:  -5.328505682361494\n",
      "Confusion matrix: \n",
      "[[12  0]\n",
      " [ 0 13]]\n",
      "Log Likelihood:  -0.19848263055374357\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data = pandas.read_excel('data.xls')\n",
    "\n",
    "X1 = data.to_numpy()[:, 0].reshape(-1, 1)\n",
    "X2 = data.to_numpy()[:, 1].reshape(-1, 1)\n",
    "X3 = data.to_numpy()[:, 2].reshape(-1, 1)\n",
    "y = data.to_numpy()[:, 3]\n",
    "\n",
    "# For logistic regression, our \"true\" case is when the value of the final exam is >=160.\n",
    "y = [1 if i >= 160 else 0 for i in y]\n",
    "\n",
    "# Iterate through different X values and fit logistic predictors based off of them\n",
    "for X in [X1, X2, X3]:\n",
    "    model = LogisticRegression(solver=\"lbfgs\")\n",
    "    model.fit(X, y)\n",
    "    y_predict = model.predict(X)\n",
    "    print(\"Confusion matrix: \")\n",
    "    print(confusion_matrix(y_predict, y))\n",
    "    \n",
    "    # Calculate likelihood\n",
    "    proba = 0\n",
    "    for p in model.predict_proba(X):\n",
    "        proba += math.log(max(p)) # For each value, sum the logs of probabilities of the predicted class (class that dataooint has the most probability of being in)\n",
    "    print(\"Log Likelihood: \", proba)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: \n",
      "[[11  1]\n",
      " [ 1 12]]\n",
      "Log Likelihood:  -2.7390000726429484\n",
      "Confusion matrix: \n",
      "[[12  0]\n",
      " [ 0 13]]\n",
      "Log Likelihood:  -0.16441273292944347\n",
      "Confusion matrix: \n",
      "[[12  0]\n",
      " [ 0 13]]\n",
      "Log Likelihood:  -0.17080621142402583\n"
     ]
    }
   ],
   "source": [
    "# Iterate through different X values and fit linear predictors based off of them\n",
    "X1X2 = data.to_numpy()[:, 0:2]\n",
    "X1X3 = data.to_numpy()[:, 0:3:2]\n",
    "X2X3 = data.to_numpy()[:, 1:3]\n",
    "\n",
    "for X in [X1X2, X1X3, X2X3]:\n",
    "    model = LogisticRegression(solver='lbfgs')\n",
    "    model.fit(X, y)\n",
    "    y_predict = model.predict(X)\n",
    "    print(\"Confusion matrix: \")\n",
    "    print(confusion_matrix(y_predict, y))\n",
    "    \n",
    "    # Calculate likelihood\n",
    "    proba = 0\n",
    "    for p in model.predict_proba(X):\n",
    "        proba += math.log(max(p)) # For each value, sum the logs of probabilities of the predicted class (class that dataooint has the most probability of being in)\n",
    "    print(\"Log Likelihood: \", proba)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: \n",
      "[[12  0]\n",
      " [ 0 13]]\n",
      "Log Likelihood:  -0.06370647181475075\n"
     ]
    }
   ],
   "source": [
    "X = data.to_numpy()[:, :]\n",
    "model = LogisticRegression(solver='lbfgs')\n",
    "model.fit(X, y)\n",
    "y_predict = model.predict(X)\n",
    "print(\"Confusion matrix: \")\n",
    "print(confusion_matrix(y_predict, y))\n",
    "\n",
    "# Calculate likelihood\n",
    "proba = 0\n",
    "for p in model.predict_proba(X):\n",
    "    proba += math.log(max(p)) # For each value, sum the logs of probabilities of the predicted class (class that dataooint has the most probability of being in)\n",
    "print(\"Log Likelihood: \", proba)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6D\n",
    "In the case of logistic regression with the threshold for passing set at 160, all models that include X3 correctly classify all points in the data set. Based just on performance, we might conclude that this means the model that uses ONLY X3 is the best, because we typically want the simplest model that performs the best. However, according to our BICs, there is still weak evidence that the classifier using all three (X1, X2, and X3) is still the best choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
